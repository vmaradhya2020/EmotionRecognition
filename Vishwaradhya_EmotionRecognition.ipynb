{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uhxGZEuIiCBy"
      },
      "source": [
        "**Emotion Recognition: Understanding Group Emotions through Individual Facial Analysis.**\n",
        "\n",
        "The Emotion Recognition code combines face detection using MTCNN (Multi-Task Cascaded Convolutional Networks) with emotion recognition using a ResNet model to analyze and annotate emotions in images of faces stored in a specified folder using a ResNet, HCB,CBAM and ESA model trained on the FER2013 dataset:\n",
        "\n",
        "a.) **RESNET**: *epochs=30 (Good Accuracy and prediction).*\n",
        "b.) **Hierarchical Convolutional Block (HCB)**: *epochs=30 (Good Accuracy and prediction).*\n",
        "c.) **Convolution Block Attention Module (CBAM)**:*epochs=2 (Bad Accuracy and prediction due to epochs=2).*\n",
        "d.) **Enhanced Spatial Attention (ESA)**:*epochs=2 (Bad Accuracy and prediction due to epochs=2).*\n",
        "\n",
        "**Summary:**\n",
        "1. Load the FER2013 dataset containing facial images and labels.\n",
        "2. Preprocess the data by extracting images and labels, normalizing pixel values, and encoding the labels.\n",
        "3. Split the dataset into training and testing sets.\n",
        "4. Load the pre-trained <ResNet> model.\n",
        "5. Evaluate the model on the testing set.\n",
        "6. Print a classification report and confusion matrix to assess the model's performance.\n",
        "7. Calculate Intersection over Union (IoU), True Positives (TP), False Positives (FP), and False Negatives (FN) for each class in the confusion matrix.\n",
        "8. Plot the confusion matrix using Plotly, showing the distribution of predicted vs. true labels.\n",
        "9. Create a visualization using Plotly to display True Positives, False Positives, and False Negatives per class, providing insights into model performance on different emotion categories.\n",
        "10. Loads a pre-trained <ResNet> model for emotion recognition from a specified file.\n",
        "11. Defines the path to a folder containing images for emotion recognition.\n",
        "12. Initializes the MTCNN (Multi-Task Cascaded Convolutional Neural Network) for face detection.\n",
        "13. Specifies a list of emotion labels - ['Angry', 'Disgust', 'Fear', 'Happy', 'Sad', 'Surprise', 'Neutral'].\n",
        "14. Iterates over files in the specified image folder.\n",
        "15. For each image file, it loads the image and performs face detection using MTCNN.\n",
        "16. Predicts emotions for detected faces in the image using the loaded <ResNet> model.\n",
        "17. Keeps track of the count of each predicted emotion for the image.\n",
        "18. Displays the image with detected faces and predicted emotions, along with bounding boxes and text labels indicating the predicted emotions.\n",
        "19. Prints the counts of each predicted emotion for the image by adding labele like ['Angry', 'Disgust', 'Fear', 'Happy', 'Sad', 'Surprise', 'Neutral'] for all images.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PBbD0egzx_4c",
        "outputId": "a1d388d6-e2e0-4036-d49f-0a4438459404"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yrY02sz9yU5l",
        "outputId": "d7819523-47e8-4125-fbba-67fc3c38a77e"
      },
      "outputs": [],
      "source": [
        "### Instaling necessary libraries\n",
        "\n",
        "!pip install mtcnn\n",
        "!pip install tensorflow\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "8858wY88ymCf",
        "outputId": "c7fa9d23-b42e-4493-c55a-8707e1d2e29c"
      },
      "outputs": [],
      "source": [
        "# RESNET\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Conv2D, BatchNormalization, Activation, MaxPooling2D, GlobalAveragePooling2D, Dropout, Dense, Add\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load the FER2013 dataset\n",
        "df = pd.read_csv(\"/content/drive/MyDrive/emo/data/fer2013.csv\")\n",
        "\n",
        "# Extract images and labels\n",
        "pixels = df[\"pixels\"].tolist()\n",
        "images = []\n",
        "for pixel_sequence in pixels:\n",
        "    face = [int(pixel) for pixel in pixel_sequence.split(' ')]\n",
        "    face = np.array(face).reshape(48, 48, 1)  # Reshape to 48x48 grayscale image\n",
        "    images.append(face.astype('float32'))\n",
        "\n",
        "X = np.array(images)\n",
        "y = df[\"emotion\"].values\n",
        "\n",
        "# Normalize the pixel values\n",
        "X /= 255.0\n",
        "\n",
        "# Encode the labels\n",
        "label_encoder = LabelEncoder()\n",
        "y = label_encoder.fit_transform(y)\n",
        "y = tf.keras.utils.to_categorical(y, num_classes=7)\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define Residual Block\n",
        "def residual_block(inputs, num_filters, strides=(1, 1), use_projection=False):\n",
        "    x = Conv2D(num_filters, kernel_size=(3, 3), strides=strides, padding='same')(inputs)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation('relu')(x)\n",
        "\n",
        "    x = Conv2D(num_filters, kernel_size=(3, 3), padding='same')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "\n",
        "    if use_projection:\n",
        "        # Use projection shortcut to match dimensions\n",
        "        shortcut = Conv2D(num_filters, kernel_size=(1, 1), strides=strides, padding='valid')(inputs)\n",
        "        shortcut = BatchNormalization()(shortcut)\n",
        "    else:\n",
        "        # Identity shortcut\n",
        "        shortcut = inputs\n",
        "\n",
        "    x = Add()([x, shortcut])\n",
        "    x = Activation('relu')(x)\n",
        "    return x\n",
        "\n",
        "# Define ResNet Model\n",
        "def ResNet(input_shape=(48, 48, 1), num_classes=7):\n",
        "    inputs = Input(shape=input_shape)\n",
        "\n",
        "    # Initial Convolutional layer\n",
        "    x = Conv2D(64, kernel_size=(3, 3), strides=(1, 1), padding='same')(inputs)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation('relu')(x)\n",
        "\n",
        "    # Residual blocks\n",
        "    x = residual_block(x, num_filters=64, use_projection=False)\n",
        "    x = residual_block(x, num_filters=64, use_projection=False)\n",
        "    x = residual_block(x, num_filters=64, use_projection=False)\n",
        "\n",
        "    x = residual_block(x, num_filters=128, strides=(2, 2), use_projection=True)\n",
        "    x = residual_block(x, num_filters=128, use_projection=False)\n",
        "    x = residual_block(x, num_filters=128, use_projection=False)\n",
        "\n",
        "    x = GlobalAveragePooling2D()(x)\n",
        "\n",
        "    # Fully connected layer\n",
        "    x = Dense(num_classes, activation='softmax')(x)\n",
        "\n",
        "    # Create model\n",
        "    model = Model(inputs, x)\n",
        "    return model\n",
        "\n",
        "# Create ResNet model\n",
        "model = ResNet()\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(X_train, y_train, epochs=30, batch_size=32, validation_data=(X_test, y_test))\n",
        "\n",
        "# Save the model\n",
        "model.save(\"/content/drive/MyDrive/emo/data/emotion_resnet.h5\")\n",
        "\n",
        "# Evaluate the model\n",
        "predictions = model.predict(X_test)\n",
        "y_pred = np.argmax(predictions, axis=1)\n",
        "\n",
        "# Print classification report and confusion matrix\n",
        "print(classification_report(np.argmax(y_test, axis=1), y_pred))\n",
        "print(confusion_matrix(np.argmax(y_test, axis=1), y_pred))\n",
        "\n",
        "# Plot accuracy and loss curves\n",
        "plt.plot(history.history['accuracy'], label='accuracy')\n",
        "plt.plot(history.history['val_accuracy'], label='val_accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "plt.plot(history.history['loss'], label='loss')\n",
        "plt.plot(history.history['val_loss'], label='val_loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "cHbHEJyxyzZq",
        "outputId": "65e8fdd5-7034-4758-df50-8761cf7f58f5"
      },
      "outputs": [],
      "source": [
        "# Confusion matrix, IOU, TP, FP, FN - RESNET.\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import plotly.express as px\n",
        "import plotly.graph_objects as go\n",
        "\n",
        "# Load the FER2013 dataset\n",
        "df = pd.read_csv(\"/content/drive/MyDrive/emo/data/fer2013.csv\")\n",
        "\n",
        "# Extract images and labels\n",
        "pixels = df[\"pixels\"].tolist()\n",
        "images = []\n",
        "for pixel_sequence in pixels:\n",
        "    face = [int(pixel) for pixel in pixel_sequence.split(' ')]\n",
        "    face = np.array(face).reshape(48, 48, 1)  # Reshape to 48x48 grayscale image\n",
        "    images.append(face.astype('float32'))\n",
        "\n",
        "X = np.array(images)\n",
        "y = df[\"emotion\"].values\n",
        "\n",
        "# Normalize the pixel values\n",
        "X /= 255.0\n",
        "\n",
        "# Encode the labels\n",
        "label_encoder = LabelEncoder()\n",
        "y = label_encoder.fit_transform(y)\n",
        "y = tf.keras.utils.to_categorical(y, num_classes=7)\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Load the saved model\n",
        "model = tf.keras.models.load_model(\"/content/drive/MyDrive/emo/data/emotion_resnet.h5\")\n",
        "\n",
        "# Evaluate the model\n",
        "predictions = model.predict(X_test)\n",
        "y_pred = np.argmax(predictions, axis=1)\n",
        "\n",
        "# Print classification report and confusion matrix\n",
        "print(classification_report(np.argmax(y_test, axis=1), y_pred))\n",
        "conf_matrix = confusion_matrix(np.argmax(y_test, axis=1), y_pred)\n",
        "print(conf_matrix)\n",
        "\n",
        "# Calculate IoU for each class\n",
        "def calculate_iou(conf_matrix):\n",
        "    ious = []\n",
        "    num_classes = conf_matrix.shape[0]\n",
        "    tp_list, fp_list, fn_list = [], [], []\n",
        "    for c in range(num_classes):\n",
        "        TP = conf_matrix[c, c]\n",
        "        FP = np.sum(conf_matrix[:, c]) - TP\n",
        "        FN = np.sum(conf_matrix[c, :]) - TP\n",
        "        IoU = TP / (TP + FP + FN)\n",
        "        ious.append(IoU)\n",
        "        tp_list.append(TP)\n",
        "        fp_list.append(FP)\n",
        "        fn_list.append(FN)\n",
        "    return ious, tp_list, fp_list, fn_list\n",
        "\n",
        "ious, tp_list, fp_list, fn_list = calculate_iou(conf_matrix)\n",
        "for i, iou in enumerate(ious):\n",
        "    print(f\"IoU for class {i}: {iou}\")\n",
        "\n",
        "# Define emotion labels\n",
        "emotion_labels = label_encoder.classes_\n",
        "\n",
        "# Plot confusion matrix using Plotly\n",
        "fig_conf_matrix = px.imshow(conf_matrix,\n",
        "                            labels=dict(x=\"Predicted Label\", y=\"True Label\", color=\"Count\"),\n",
        "                            x=emotion_labels,\n",
        "                            y=emotion_labels,\n",
        "                            title=\"Confusion Matrix\")\n",
        "fig_conf_matrix.update_layout(coloraxis_showscale=True)\n",
        "fig_conf_matrix.show()\n",
        "\n",
        "# Plot TP, FP, FN using Plotly\n",
        "metrics_data = pd.DataFrame({\n",
        "    \"Class\": emotion_labels,\n",
        "    \"True Positives (TP)\": tp_list,\n",
        "    \"False Positives (FP)\": fp_list,\n",
        "    \"False Negatives (FN)\": fn_list\n",
        "})\n",
        "\n",
        "fig_metrics = go.Figure()\n",
        "fig_metrics.add_trace(go.Bar(x=metrics_data[\"Class\"], y=metrics_data[\"True Positives (TP)\"], name='TP'))\n",
        "fig_metrics.add_trace(go.Bar(x=metrics_data[\"Class\"], y=metrics_data[\"False Positives (FP)\"], name='FP'))\n",
        "fig_metrics.add_trace(go.Bar(x=metrics_data[\"Class\"], y=metrics_data[\"False Negatives (FN)\"], name='FN'))\n",
        "\n",
        "fig_metrics.update_layout(\n",
        "    barmode='group',\n",
        "    title=\"True Positives, False Positives, and False Negatives per Class\",\n",
        "    xaxis_title=\"Class\",\n",
        "    yaxis_title=\"Count\",\n",
        "    legend_title=\"Metrics\"\n",
        ")\n",
        "\n",
        "fig_metrics.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "zEAbVacgzDMG",
        "outputId": "fd0b9501-04d7-434a-b5d0-cc9d95560fa3"
      },
      "outputs": [],
      "source": [
        "# Detecting Group emotions - RESNET.\n",
        "\n",
        "import cv2\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from mtcnn import MTCNN\n",
        "from google.colab.patches import cv2_imshow\n",
        "import os\n",
        "\n",
        "\n",
        "# Load the model with custom objects\n",
        "model = tf.keras.models.load_model(\"/content/drive/MyDrive/emo/data/emotion_resnet.h5\", custom_objects={'SpatialAttention': SpatialAttention})\n",
        "\n",
        "# Path to the folder containing images\n",
        "image_folder = \"/content/drive/MyDrive/emo/data\"\n",
        "\n",
        "# Initialize MTCNN for face detection\n",
        "detector = MTCNN()\n",
        "\n",
        "# Emotion labels\n",
        "emotion_labels = ['Angry', 'Disgust', 'Fear', 'Happy', 'Sad', 'Surprise', 'Neutral']\n",
        "\n",
        "# Iterate over files in the folder\n",
        "for filename in os.listdir(image_folder):\n",
        "    # Check if the file is an image file\n",
        "    if filename.endswith((\".jpg\", \".jpeg\", \".png\")):\n",
        "        # Construct the full path to the image file\n",
        "        image_path = os.path.join(image_folder, filename)\n",
        "\n",
        "        # Load the input image\n",
        "        image = cv2.imread(image_path)\n",
        "\n",
        "        # Perform face detection using MTCNN\n",
        "        detections = detector.detect_faces(image)\n",
        "\n",
        "        # Initialize emotion counters\n",
        "        emotion_counts = {emotion: 0 for emotion in emotion_labels}\n",
        "\n",
        "        # Predict emotions for the detected faces\n",
        "        for detection in detections:\n",
        "            x, y, w, h = detection['box']\n",
        "            face_roi = image[y:y+h, x:x+w]\n",
        "            face = cv2.resize(face_roi, (48, 48))  # Resize to 48x48 for emotion recognition\n",
        "            face = cv2.cvtColor(face, cv2.COLOR_BGR2GRAY)  # Convert to grayscale\n",
        "            face = face.astype('float32') / 255.0  # Normalize pixel values\n",
        "            face = np.expand_dims(face, axis=0)  # Add batch dimension\n",
        "            face = np.expand_dims(face, axis=-1)  # Add channel dimension\n",
        "            prediction = model.predict(face)\n",
        "            emotion_index = np.argmax(prediction)\n",
        "            emotion = emotion_labels[emotion_index]\n",
        "            emotion_counts[emotion] += 1\n",
        "            cv2.putText(image, emotion, (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
        "            cv2.rectangle(image, (x, y), (x+w, y+h), (255, 0, 0), 2)\n",
        "\n",
        "        # Print emotion counts for the image\n",
        "        print(f\"Image: {filename}\")\n",
        "        for emotion, count in emotion_counts.items():\n",
        "            print(f\"Number of {emotion}: {count}\")\n",
        "\n",
        "        # Display the image with predicted emotions\n",
        "        cv2_imshow(image)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "21h3VNbY60C_"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "wswLNcrO_9JN",
        "outputId": "f72df91b-5f6e-450a-daa1-96c731512dd9"
      },
      "outputs": [],
      "source": [
        "# Hierarchical Convolutional Block\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, Activation, Multiply\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.utils import plot_model\n",
        "from keras.models import Model\n",
        "from keras.layers import Input, Dense, Flatten, Dropout, BatchNormalization, concatenate\n",
        "from keras.optimizers import Adam, SGD\n",
        "from keras.regularizers import l1, l2\n",
        "\n",
        "\n",
        "# Load the FER2013 dataset\n",
        "df = pd.read_csv(\"/content/drive/MyDrive/emo/data/fer2013.csv\")\n",
        "\n",
        "# Extract images and labels\n",
        "pixels = df[\"pixels\"].tolist()\n",
        "images = []\n",
        "for pixel_sequence in pixels:\n",
        "    face = [int(pixel) for pixel in pixel_sequence.split(' ')]\n",
        "    face = np.array(face).reshape(48, 48, 1)  # Reshape to 48x48 grayscale image\n",
        "    images.append(face.astype('float32'))\n",
        "\n",
        "X = np.array(images)\n",
        "y = df[\"emotion\"].values\n",
        "\n",
        "# Normalize the pixel values\n",
        "X /= 255.0\n",
        "\n",
        "# Encode the labels\n",
        "label_encoder = LabelEncoder()\n",
        "y = label_encoder.fit_transform(y)\n",
        "y = tf.keras.utils.to_categorical(y, num_classes=7)\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define the CNN model with attention\n",
        "def FER_Model(input_shape=(48, 48, 1)):\n",
        "    visible = Input(shape=input_shape, name='input')\n",
        "    num_classes = 7\n",
        "\n",
        "    # The first block\n",
        "    conv1_1 = Conv2D(64, kernel_size=3, activation='relu', padding='same', name='conv1_1')(visible)\n",
        "    conv1_1 = BatchNormalization()(conv1_1)\n",
        "    conv1_2 = Conv2D(64, kernel_size=3, activation='relu', padding='same', name='conv1_2')(conv1_1)\n",
        "    conv1_2 = BatchNormalization()(conv1_2)\n",
        "    pool1_1 = MaxPooling2D(pool_size=(2, 2), name='pool1_1')(conv1_2)\n",
        "    drop1_1 = Dropout(0.3, name='drop1_1')(pool1_1)\n",
        "\n",
        "    # The second block\n",
        "    conv2_1 = Conv2D(128, kernel_size=3, activation='relu', padding='same', name='conv2_1')(drop1_1)\n",
        "    conv2_1 = BatchNormalization()(conv2_1)\n",
        "    conv2_2 = Conv2D(128, kernel_size=3, activation='relu', padding='same', name='conv2_2')(conv2_1)\n",
        "    conv2_2 = BatchNormalization()(conv2_2)\n",
        "    conv2_3 = Conv2D(128, kernel_size=3, activation='relu', padding='same', name='conv2_3')(conv2_2)\n",
        "    conv2_3 = BatchNormalization()(conv2_3)\n",
        "    pool2_1 = MaxPooling2D(pool_size=(2, 2), name='pool2_1')(conv2_3)\n",
        "    drop2_1 = Dropout(0.3, name='drop2_1')(pool2_1)\n",
        "\n",
        "    # The third block\n",
        "    conv3_1 = Conv2D(256, kernel_size=3, activation='relu', padding='same', name='conv3_1')(drop2_1)\n",
        "    conv3_1 = BatchNormalization()(conv3_1)\n",
        "    conv3_2 = Conv2D(256, kernel_size=3, activation='relu', padding='same', name='conv3_2')(conv3_1)\n",
        "    conv3_2 = BatchNormalization()(conv3_2)\n",
        "    conv3_3 = Conv2D(256, kernel_size=3, activation='relu', padding='same', name='conv3_3')(conv3_2)\n",
        "    conv3_3 = BatchNormalization()(conv3_3)\n",
        "    conv3_4 = Conv2D(256, kernel_size=3, activation='relu', padding='same', name='conv3_4')(conv3_3)\n",
        "    conv3_4 = BatchNormalization()(conv3_4)\n",
        "    pool3_1 = MaxPooling2D(pool_size=(2, 2), name='pool3_1')(conv3_4)\n",
        "    drop3_1 = Dropout(0.3, name='drop3_1')(pool3_1)\n",
        "\n",
        "    # The fourth block\n",
        "    conv4_1 = Conv2D(256, kernel_size=3, activation='relu', padding='same', name='conv4_1')(drop3_1)\n",
        "    conv4_1 = BatchNormalization()(conv4_1)\n",
        "    conv4_2 = Conv2D(256, kernel_size=3, activation='relu', padding='same', name='conv4_2')(conv4_1)\n",
        "    conv4_2 = BatchNormalization()(conv4_2)\n",
        "    conv4_3 = Conv2D(256, kernel_size=3, activation='relu', padding='same', name='conv4_3')(conv4_2)\n",
        "    conv4_3 = BatchNormalization()(conv4_3)\n",
        "    conv4_4 = Conv2D(256, kernel_size=3, activation='relu', padding='same', name='conv4_4')(conv4_3)\n",
        "    conv4_4 = BatchNormalization()(conv4_4)\n",
        "    pool4_1 = MaxPooling2D(pool_size=(2, 2), name='pool4_1')(conv4_4)\n",
        "    drop4_1 = Dropout(0.3, name='drop4_1')(pool4_1)\n",
        "\n",
        "    # The fifth block\n",
        "    conv5_1 = Conv2D(512, kernel_size=3, activation='relu', padding='same', name='conv5_1')(drop4_1)\n",
        "    conv5_1 = BatchNormalization()(conv5_1)\n",
        "    conv5_2 = Conv2D(512, kernel_size=3, activation='relu', padding='same', name='conv5_2')(conv5_1)\n",
        "    conv5_2 = BatchNormalization()(conv5_2)\n",
        "    conv5_3 = Conv2D(512, kernel_size=3, activation='relu', padding='same', name='conv5_3')(conv5_2)\n",
        "    conv5_3 = BatchNormalization()(conv5_3)\n",
        "    conv5_4 = Conv2D(512, kernel_size=3, activation='relu', padding='same', name='conv5_4')(conv5_3)\n",
        "    conv5_4 = BatchNormalization()(conv5_4)\n",
        "    pool5_1 = MaxPooling2D(pool_size=(2, 2), name='pool5_1')(conv5_4)\n",
        "    drop5_1 = Dropout(0.3, name='drop5_1')(pool5_1)\n",
        "\n",
        "    # Flatten and output\n",
        "    flatten = Flatten(name='flatten')(drop5_1)\n",
        "    output = Dense(num_classes, activation='softmax', name='output')(flatten)\n",
        "\n",
        "    # Create model\n",
        "    model = Model(inputs=visible, outputs=output)\n",
        "\n",
        "    return model\n",
        "\n",
        "# Create an instance of the model\n",
        "model = FER_Model()\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(X_train, y_train, epochs=50, batch_size=32, validation_data=(X_test, y_test))\n",
        "\n",
        "# Save the model\n",
        "model.save(\"/content/drive/MyDrive/emo/data/emotion_Hierarchical.h5\")\n",
        "\n",
        "# Evaluate the model\n",
        "predictions = model.predict(X_test)\n",
        "y_pred = np.argmax(predictions, axis=1)\n",
        "\n",
        "# Print classification report and confusion matrix\n",
        "print(classification_report(np.argmax(y_test, axis=1), y_pred))\n",
        "print(confusion_matrix(np.argmax(y_test, axis=1), y_pred))\n",
        "\n",
        "# Plot accuracy and loss curves\n",
        "plt.plot(history.history['accuracy'], label='accuracy')\n",
        "plt.plot(history.history['val_accuracy'], label='val_accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "plt.plot(history.history['loss'], label='loss')\n",
        "plt.plot(history.history['val_loss'], label='val_loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "LBIKg0_SRgly",
        "outputId": "06f7e99b-f2b9-4a35-f3bd-981f2079a4c6"
      },
      "outputs": [],
      "source": [
        "# Confusion matrix, IOU, TP, FP, FN - Hierarchical.\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import plotly.express as px\n",
        "import plotly.graph_objects as go\n",
        "\n",
        "# Load the FER2013 dataset\n",
        "df = pd.read_csv(\"/content/drive/MyDrive/emo/data/fer2013.csv\")\n",
        "\n",
        "# Extract images and labels\n",
        "pixels = df[\"pixels\"].tolist()\n",
        "images = []\n",
        "for pixel_sequence in pixels:\n",
        "    face = [int(pixel) for pixel in pixel_sequence.split(' ')]\n",
        "    face = np.array(face).reshape(48, 48, 1)  # Reshape to 48x48 grayscale image\n",
        "    images.append(face.astype('float32'))\n",
        "\n",
        "X = np.array(images)\n",
        "y = df[\"emotion\"].values\n",
        "\n",
        "# Normalize the pixel values\n",
        "X /= 255.0\n",
        "\n",
        "# Encode the labels\n",
        "label_encoder = LabelEncoder()\n",
        "y = label_encoder.fit_transform(y)\n",
        "y = tf.keras.utils.to_categorical(y, num_classes=7)\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Load the saved model\n",
        "model = tf.keras.models.load_model(\"/content/drive/MyDrive/emo/data/emotion_Hierarchical.h5\")\n",
        "\n",
        "# Evaluate the model\n",
        "predictions = model.predict(X_test)\n",
        "y_pred = np.argmax(predictions, axis=1)\n",
        "\n",
        "# Print classification report and confusion matrix\n",
        "print(classification_report(np.argmax(y_test, axis=1), y_pred))\n",
        "conf_matrix = confusion_matrix(np.argmax(y_test, axis=1), y_pred)\n",
        "print(conf_matrix)\n",
        "\n",
        "# Calculate IoU for each class\n",
        "def calculate_iou(conf_matrix):\n",
        "    ious = []\n",
        "    num_classes = conf_matrix.shape[0]\n",
        "    tp_list, fp_list, fn_list = [], [], []\n",
        "    for c in range(num_classes):\n",
        "        TP = conf_matrix[c, c]\n",
        "        FP = np.sum(conf_matrix[:, c]) - TP\n",
        "        FN = np.sum(conf_matrix[c, :]) - TP\n",
        "        IoU = TP / (TP + FP + FN)\n",
        "        ious.append(IoU)\n",
        "        tp_list.append(TP)\n",
        "        fp_list.append(FP)\n",
        "        fn_list.append(FN)\n",
        "    return ious, tp_list, fp_list, fn_list\n",
        "\n",
        "ious, tp_list, fp_list, fn_list = calculate_iou(conf_matrix)\n",
        "for i, iou in enumerate(ious):\n",
        "    print(f\"IoU for class {i}: {iou}\")\n",
        "\n",
        "# Define emotion labels\n",
        "emotion_labels = label_encoder.classes_\n",
        "\n",
        "# Plot confusion matrix using Plotly\n",
        "fig_conf_matrix = px.imshow(conf_matrix,\n",
        "                            labels=dict(x=\"Predicted Label\", y=\"True Label\", color=\"Count\"),\n",
        "                            x=emotion_labels,\n",
        "                            y=emotion_labels,\n",
        "                            title=\"Confusion Matrix\")\n",
        "fig_conf_matrix.update_layout(coloraxis_showscale=True)\n",
        "fig_conf_matrix.show()\n",
        "\n",
        "# Plot TP, FP, FN using Plotly\n",
        "metrics_data = pd.DataFrame({\n",
        "    \"Class\": emotion_labels,\n",
        "    \"True Positives (TP)\": tp_list,\n",
        "    \"False Positives (FP)\": fp_list,\n",
        "    \"False Negatives (FN)\": fn_list\n",
        "})\n",
        "\n",
        "fig_metrics = go.Figure()\n",
        "fig_metrics.add_trace(go.Bar(x=metrics_data[\"Class\"], y=metrics_data[\"True Positives (TP)\"], name='TP'))\n",
        "fig_metrics.add_trace(go.Bar(x=metrics_data[\"Class\"], y=metrics_data[\"False Positives (FP)\"], name='FP'))\n",
        "fig_metrics.add_trace(go.Bar(x=metrics_data[\"Class\"], y=metrics_data[\"False Negatives (FN)\"], name='FN'))\n",
        "\n",
        "fig_metrics.update_layout(\n",
        "    barmode='group',\n",
        "    title=\"True Positives, False Positives, and False Negatives per Class\",\n",
        "    xaxis_title=\"Class\",\n",
        "    yaxis_title=\"Count\",\n",
        "    legend_title=\"Metrics\"\n",
        ")\n",
        "\n",
        "fig_metrics.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "W5zYLaE-R59G",
        "outputId": "d6e63f76-1ca5-4f87-f76f-fe96a414d64d"
      },
      "outputs": [],
      "source": [
        "# Detecting Group emotions - Hierarchical\n",
        "\n",
        "import cv2\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from mtcnn import MTCNN\n",
        "from google.colab.patches import cv2_imshow\n",
        "import os\n",
        "\n",
        "\n",
        "# Load the model with custom objects\n",
        "# model = tf.keras.models.load_model(\"/content/drive/MyDrive/emo/data/emotion_Hierarchical.h5\", custom_objects={'SpatialAttention': SpatialAttention})\n",
        "model = tf.keras.models.load_model(\"/content/drive/MyDrive/emo/data/emotion_Hierarchical.h5\")\n",
        "\n",
        "\n",
        "# Path to the folder containing images\n",
        "image_folder = \"/content/drive/MyDrive/emo/data\"\n",
        "\n",
        "# Initialize MTCNN for face detection\n",
        "detector = MTCNN()\n",
        "\n",
        "# Emotion labels\n",
        "emotion_labels = ['Angry', 'Disgust', 'Fear', 'Happy', 'Sad', 'Surprise', 'Neutral']\n",
        "\n",
        "# Iterate over files in the folder\n",
        "for filename in os.listdir(image_folder):\n",
        "    # Check if the file is an image file\n",
        "    if filename.endswith((\".jpg\", \".jpeg\", \".png\")):\n",
        "        # Construct the full path to the image file\n",
        "        image_path = os.path.join(image_folder, filename)\n",
        "\n",
        "        # Load the input image\n",
        "        image = cv2.imread(image_path)\n",
        "\n",
        "        # Perform face detection using MTCNN\n",
        "        detections = detector.detect_faces(image)\n",
        "\n",
        "        # Initialize emotion counters\n",
        "        emotion_counts = {emotion: 0 for emotion in emotion_labels}\n",
        "\n",
        "        # Predict emotions for the detected faces\n",
        "        for detection in detections:\n",
        "            x, y, w, h = detection['box']\n",
        "            face_roi = image[y:y+h, x:x+w]\n",
        "            face = cv2.resize(face_roi, (48, 48))  # Resize to 48x48 for emotion recognition\n",
        "            face = cv2.cvtColor(face, cv2.COLOR_BGR2GRAY)  # Convert to grayscale\n",
        "            face = face.astype('float32') / 255.0  # Normalize pixel values\n",
        "            face = np.expand_dims(face, axis=0)  # Add batch dimension\n",
        "            face = np.expand_dims(face, axis=-1)  # Add channel dimension\n",
        "            prediction = model.predict(face)\n",
        "            emotion_index = np.argmax(prediction)\n",
        "            emotion = emotion_labels[emotion_index]\n",
        "            emotion_counts[emotion] += 1\n",
        "            cv2.putText(image, emotion, (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
        "            cv2.rectangle(image, (x, y), (x+w, y+h), (255, 0, 0), 2)\n",
        "\n",
        "        # Print emotion counts for the image\n",
        "        print(f\"Image: {filename}\")\n",
        "        for emotion, count in emotion_counts.items():\n",
        "            print(f\"Number of {emotion}: {count}\")\n",
        "\n",
        "        # Display the image with predicted emotions\n",
        "        cv2_imshow(image)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "ODUvxgePVFXo",
        "outputId": "86418ef4-5319-41eb-cb3e-ca8b37919ca9"
      },
      "outputs": [],
      "source": [
        "# Convolution Block Attention Module (CBAM).\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, Activation, Multiply, Add\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.utils import plot_model\n",
        "from keras.models import Model\n",
        "from keras.layers import Input, Dense, Flatten, Dropout, BatchNormalization, concatenate, GlobalAveragePooling2D, Reshape\n",
        "from keras.optimizers import Adam, SGD\n",
        "from keras.regularizers import l1, l2\n",
        "from keras.layers import Lambda\n",
        "from keras import backend as K\n",
        "from keras.layers import Lambda, Concatenate\n",
        "from keras import backend as K\n",
        "\n",
        "# Define the Convolutional Block Attention Module (CBAM)\n",
        "def cbam_block(cbam_feature, ratio=8):\n",
        "    cbam_feature = channel_attention(cbam_feature, ratio)\n",
        "    cbam_feature = spatial_attention(cbam_feature)\n",
        "    return cbam_feature\n",
        "\n",
        "def channel_attention(input_feature, ratio=8):\n",
        "    channel = input_feature.shape[-1]\n",
        "    shared_layer_one = Dense(channel // ratio,\n",
        "                             activation='relu',\n",
        "                             kernel_initializer='he_normal',\n",
        "                             use_bias=True,\n",
        "                             bias_initializer='zeros')\n",
        "    shared_layer_two = Dense(channel,\n",
        "                             kernel_initializer='he_normal',\n",
        "                             use_bias=True,\n",
        "                             bias_initializer='zeros')\n",
        "\n",
        "    avg_pool = GlobalAveragePooling2D()(input_feature)\n",
        "    avg_pool = Reshape((1, 1, channel))(avg_pool)\n",
        "    assert avg_pool.shape[1:] == (1, 1, channel)\n",
        "    avg_pool = shared_layer_one(avg_pool)\n",
        "    assert avg_pool.shape[1:] == (1, 1, channel // ratio)\n",
        "    avg_pool = shared_layer_two(avg_pool)\n",
        "    assert avg_pool.shape[1:] == (1, 1, channel)\n",
        "\n",
        "    max_pool = tf.keras.layers.GlobalMaxPooling2D()(input_feature)\n",
        "    max_pool = Reshape((1, 1, channel))(max_pool)\n",
        "    assert max_pool.shape[1:] == (1, 1, channel)\n",
        "    max_pool = shared_layer_one(max_pool)\n",
        "    assert max_pool.shape[1:] == (1, 1, channel // ratio)\n",
        "    max_pool = shared_layer_two(max_pool)\n",
        "    assert max_pool.shape[1:] == (1, 1, channel)\n",
        "\n",
        "    cbam_feature = Add()([avg_pool, max_pool])\n",
        "    cbam_feature = Activation('sigmoid')(cbam_feature)\n",
        "\n",
        "    return Multiply()([input_feature, cbam_feature])\n",
        "\n",
        "def spatial_attention(input_feature):\n",
        "    cbam_feature = input_feature\n",
        "\n",
        "    avg_pool = Lambda(lambda x: K.mean(x, axis=3, keepdims=True))(cbam_feature)\n",
        "    assert avg_pool.shape[-1] == 1\n",
        "    max_pool = Lambda(lambda x: K.max(x, axis=3, keepdims=True))(cbam_feature)\n",
        "    assert max_pool.shape[-1] == 1\n",
        "    concat = Concatenate(axis=3)([avg_pool, max_pool])\n",
        "    assert concat.shape[-1] == 2\n",
        "    cbam_feature = Conv2D(filters=1,\n",
        "                          kernel_size=7, strides=1,\n",
        "                          padding='same',\n",
        "                          activation='sigmoid',\n",
        "                          kernel_initializer='he_normal',\n",
        "                          use_bias=False)(concat)\n",
        "\n",
        "    return cbam_feature\n",
        "\n",
        "# Load the FER2013 dataset\n",
        "df = pd.read_csv(\"/content/drive/MyDrive/emo/data/fer2013.csv\")\n",
        "\n",
        "# Extract images and labels\n",
        "pixels = df[\"pixels\"].tolist()\n",
        "images = []\n",
        "for pixel_sequence in pixels:\n",
        "    face = [int(pixel) for pixel in pixel_sequence.split(' ')]\n",
        "    face = np.array(face).reshape(48, 48, 1)  # Reshape to 48x48 grayscale image\n",
        "    images.append(face.astype('float32'))\n",
        "\n",
        "X = np.array(images)\n",
        "y = df[\"emotion\"].values\n",
        "\n",
        "# Normalize the pixel values\n",
        "X /= 255.0\n",
        "\n",
        "# Encode the labels\n",
        "label_encoder = LabelEncoder()\n",
        "y = label_encoder.fit_transform(y)\n",
        "y = tf.keras.utils.to_categorical(y, num_classes=7)\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define the CNN model with CBAM\n",
        "def FER_Model(input_shape=(48, 48, 1)):\n",
        "    visible = Input(shape=input_shape, name='input')\n",
        "    num_classes = 7\n",
        "\n",
        "    # The first block\n",
        "    conv1_1 = Conv2D(64, kernel_size=3, activation='relu', padding='same', name='conv1_1')(visible)\n",
        "    conv1_1 = BatchNormalization()(conv1_1)\n",
        "    conv1_1 = cbam_block(conv1_1)  # Add CBAM here\n",
        "    conv1_2 = Conv2D(64, kernel_size=3, activation='relu', padding='same', name='conv1_2')(conv1_1)\n",
        "    conv1_2 = BatchNormalization()(conv1_2)\n",
        "    conv1_2 = cbam_block(conv1_2)  # Add CBAM here\n",
        "    pool1_1 = MaxPooling2D(pool_size=(2, 2), name='pool1_1')(conv1_2)\n",
        "    drop1_1 = Dropout(0.3, name='drop1_1')(pool1_1)\n",
        "\n",
        "     # The second block\n",
        "    conv2_1 = Conv2D(128, kernel_size=3, activation='relu', padding='same', name='conv2_1')(drop1_1)\n",
        "    conv2_1 = BatchNormalization()(conv2_1)\n",
        "    conv2_1 = cbam_block(conv2_1)  # Add CBAM here\n",
        "    conv2_2 = Conv2D(128, kernel_size=3, activation='relu', padding='same', name='conv2_2')(conv2_1)\n",
        "    conv2_2 = BatchNormalization()(conv2_2)\n",
        "    conv2_2 = cbam_block(conv2_2)  # Add CBAM here\n",
        "    conv2_3 = Conv2D(128, kernel_size=3, activation='relu', padding='same', name='conv2_3')(conv2_2)\n",
        "    conv2_3 = BatchNormalization()(conv2_3)\n",
        "    conv2_3 = cbam_block(conv2_3)  # Add CBAM here\n",
        "    pool2_1 = MaxPooling2D(pool_size=(2, 2), name='pool2_1')(conv2_3)\n",
        "    drop2_1 = Dropout(0.3, name='drop2_1')(pool2_1)\n",
        "\n",
        "    # The third block\n",
        "    conv3_1 = Conv2D(256, kernel_size=3, activation='relu', padding='same', name='conv3_1')(drop2_1)\n",
        "    conv3_1 = BatchNormalization()(conv3_1)\n",
        "    conv3_2 = Conv2D(256, kernel_size=3, activation='relu', padding='same', name='conv3_2')(conv3_1)\n",
        "    conv3_2 = BatchNormalization()(conv3_2)\n",
        "    conv3_3 = Conv2D(256, kernel_size=3, activation='relu', padding='same', name='conv3_3')(conv3_2)\n",
        "    conv3_3 = BatchNormalization()(conv3_3)\n",
        "    pool3_1 = MaxPooling2D(pool_size=(2, 2), name='pool3_1')(conv3_3)\n",
        "    drop3_1 = Dropout(0.3, name='drop3_1')(pool3_1)\n",
        "\n",
        "    # The fourth block\n",
        "    conv4_1 = Conv2D(256, kernel_size=3, activation='relu', padding='same', name='conv4_1')(drop3_1)\n",
        "    conv4_1 = BatchNormalization()(conv4_1)\n",
        "    conv4_2 = Conv2D(256, kernel_size=3, activation='relu', padding='same', name='conv4_2')(conv4_1)\n",
        "    conv4_2 = BatchNormalization()(conv4_2)\n",
        "    conv4_3 = Conv2D(256, kernel_size=3, activation='relu', padding='same', name='conv4_3')(conv4_2)\n",
        "    conv4_3 = BatchNormalization()(conv4_3)\n",
        "    pool4_1 = MaxPooling2D(pool_size=(2, 2), name='pool4_1')(conv4_3)\n",
        "    drop4_1 = Dropout(0.3, name='drop4_1')(pool4_1)\n",
        "\n",
        "    # The fifth block\n",
        "    conv5_1 = Conv2D(512, kernel_size=3, activation='relu', padding='same', name='conv5_1')(drop4_1)\n",
        "    conv5_1 = BatchNormalization()(conv5_1)\n",
        "    conv5_2 = Conv2D(512, kernel_size=3, activation='relu', padding='same', name='conv5_2')(conv5_1)\n",
        "    conv5_2 = BatchNormalization()(conv5_2)\n",
        "    conv5_3 = Conv2D(512, kernel_size=3, activation='relu', padding='same', name='conv5_3')(conv5_2)\n",
        "    conv5_3 = BatchNormalization()(conv5_3)\n",
        "    pool5_1 = MaxPooling2D(pool_size=(2, 2), name='pool5_1')(conv5_3)\n",
        "    drop5_1 = Dropout(0.3, name='drop5_1')(pool5_1)\n",
        "\n",
        "    # Global average pooling\n",
        "    global_avg_pool = GlobalAveragePooling2D()(drop5_1)\n",
        "\n",
        "    # Output layer\n",
        "    output = Dense(num_classes, activation='softmax', name='output')(global_avg_pool)\n",
        "\n",
        "    # Create model\n",
        "    model = Model(inputs=visible, outputs=output)\n",
        "\n",
        "    return model\n",
        "\n",
        "# Create an instance of the model\n",
        "model = FER_Model()\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(X_train, y_train, epochs=2, batch_size=32, validation_data=(X_test, y_test))\n",
        "\n",
        "# Save the model\n",
        "model.save(\"/content/drive/MyDrive/emo/data/emotion_3_cbam.h5\")\n",
        "\n",
        "# Evaluate the model\n",
        "predictions = model.predict(X_test)\n",
        "y_pred = np.argmax(predictions, axis=1)\n",
        "\n",
        "# Print classification report and confusion matrix\n",
        "print(classification_report(np.argmax(y_test, axis=1), y_pred))\n",
        "print(confusion_matrix(np.argmax(y_test, axis=1), y_pred))\n",
        "\n",
        "# Plot accuracy and loss curves\n",
        "plt.plot(history.history['accuracy'], label='accuracy')\n",
        "plt.plot(history.history['val_accuracy'], label='val_accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "plt.plot(history.history['loss'], label='loss')\n",
        "plt.plot(history.history['val_loss'], label='val_loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "OYRwqWR0m2hK",
        "outputId": "bb6b496c-0b58-462e-e244-f4bf94f99d27"
      },
      "outputs": [],
      "source": [
        "# Confusion matrix, IOU, TP, FP, FN - Convolution Block Attention Module (CBAM).\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import plotly.express as px\n",
        "import plotly.graph_objects as go\n",
        "\n",
        "# Load the FER2013 dataset\n",
        "df = pd.read_csv(\"/content/drive/MyDrive/emo/data/fer2013.csv\")\n",
        "\n",
        "# Extract images and labels\n",
        "pixels = df[\"pixels\"].tolist()\n",
        "images = []\n",
        "for pixel_sequence in pixels:\n",
        "    face = [int(pixel) for pixel in pixel_sequence.split(' ')]\n",
        "    face = np.array(face).reshape(48, 48, 1)  # Reshape to 48x48 grayscale image\n",
        "    images.append(face.astype('float32'))\n",
        "\n",
        "X = np.array(images)\n",
        "y = df[\"emotion\"].values\n",
        "\n",
        "# Normalize the pixel values\n",
        "X /= 255.0\n",
        "\n",
        "# Encode the labels\n",
        "label_encoder = LabelEncoder()\n",
        "y = label_encoder.fit_transform(y)\n",
        "y = tf.keras.utils.to_categorical(y, num_classes=7)\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Load the saved model\n",
        "model = tf.keras.models.load_model(\"/content/drive/MyDrive/emo/data/emotion_cbam.h5\")\n",
        "\n",
        "# Evaluate the model\n",
        "predictions = model.predict(X_test)\n",
        "y_pred = np.argmax(predictions, axis=1)\n",
        "\n",
        "# Print classification report and confusion matrix\n",
        "print(classification_report(np.argmax(y_test, axis=1), y_pred))\n",
        "conf_matrix = confusion_matrix(np.argmax(y_test, axis=1), y_pred)\n",
        "print(conf_matrix)\n",
        "\n",
        "# Calculate IoU for each class\n",
        "def calculate_iou(conf_matrix):\n",
        "    ious = []\n",
        "    num_classes = conf_matrix.shape[0]\n",
        "    tp_list, fp_list, fn_list = [], [], []\n",
        "    for c in range(num_classes):\n",
        "        TP = conf_matrix[c, c]\n",
        "        FP = np.sum(conf_matrix[:, c]) - TP\n",
        "        FN = np.sum(conf_matrix[c, :]) - TP\n",
        "        IoU = TP / (TP + FP + FN)\n",
        "        ious.append(IoU)\n",
        "        tp_list.append(TP)\n",
        "        fp_list.append(FP)\n",
        "        fn_list.append(FN)\n",
        "    return ious, tp_list, fp_list, fn_list\n",
        "\n",
        "ious, tp_list, fp_list, fn_list = calculate_iou(conf_matrix)\n",
        "for i, iou in enumerate(ious):\n",
        "    print(f\"IoU for class {i}: {iou}\")\n",
        "\n",
        "# Define emotion labels\n",
        "emotion_labels = label_encoder.classes_\n",
        "\n",
        "# Plot confusion matrix using Plotly\n",
        "fig_conf_matrix = px.imshow(conf_matrix,\n",
        "                            labels=dict(x=\"Predicted Label\", y=\"True Label\", color=\"Count\"),\n",
        "                            x=emotion_labels,\n",
        "                            y=emotion_labels,\n",
        "                            title=\"Confusion Matrix\")\n",
        "fig_conf_matrix.update_layout(coloraxis_showscale=True)\n",
        "fig_conf_matrix.show()\n",
        "\n",
        "# Plot TP, FP, FN using Plotly\n",
        "metrics_data = pd.DataFrame({\n",
        "    \"Class\": emotion_labels,\n",
        "    \"True Positives (TP)\": tp_list,\n",
        "    \"False Positives (FP)\": fp_list,\n",
        "    \"False Negatives (FN)\": fn_list\n",
        "})\n",
        "\n",
        "fig_metrics = go.Figure()\n",
        "fig_metrics.add_trace(go.Bar(x=metrics_data[\"Class\"], y=metrics_data[\"True Positives (TP)\"], name='TP'))\n",
        "fig_metrics.add_trace(go.Bar(x=metrics_data[\"Class\"], y=metrics_data[\"False Positives (FP)\"], name='FP'))\n",
        "fig_metrics.add_trace(go.Bar(x=metrics_data[\"Class\"], y=metrics_data[\"False Negatives (FN)\"], name='FN'))\n",
        "\n",
        "fig_metrics.update_layout(\n",
        "    barmode='group',\n",
        "    title=\"True Positives, False Positives, and False Negatives per Class\",\n",
        "    xaxis_title=\"Class\",\n",
        "    yaxis_title=\"Count\",\n",
        "    legend_title=\"Metrics\"\n",
        ")\n",
        "\n",
        "fig_metrics.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "pr7uglea9Lej",
        "outputId": "67c1b787-754a-4d38-9d2b-d9e73d8dd8cd"
      },
      "outputs": [],
      "source": [
        "# Detecting Group emotions - Convolution Block Attention Module (CBAM).\n",
        "\n",
        "import cv2\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from mtcnn import MTCNN\n",
        "from google.colab.patches import cv2_imshow\n",
        "import os\n",
        "\n",
        "\n",
        "# Load the model with custom objects\n",
        "# model = tf.keras.models.load_model(\"/content/drive/MyDrive/emo/data/emotion_cbam.h5\", custom_objects={'SpatialAttention': SpatialAttention})\n",
        "model = tf.keras.models.load_model(\"/content/drive/MyDrive/emo/data/emotion_cbam.h5\")\n",
        "\n",
        "\n",
        "# Path to the folder containing images\n",
        "image_folder = \"/content/drive/MyDrive/emo/data\"\n",
        "\n",
        "# Initialize MTCNN for face detection\n",
        "detector = MTCNN()\n",
        "\n",
        "# Emotion labels\n",
        "emotion_labels = ['Angry', 'Disgust', 'Fear', 'Happy', 'Sad', 'Surprise', 'Neutral']\n",
        "\n",
        "# Iterate over files in the folder\n",
        "for filename in os.listdir(image_folder):\n",
        "    # Check if the file is an image file\n",
        "    if filename.endswith((\".jpg\", \".jpeg\", \".png\")):\n",
        "        # Construct the full path to the image file\n",
        "        image_path = os.path.join(image_folder, filename)\n",
        "\n",
        "        # Load the input image\n",
        "        image = cv2.imread(image_path)\n",
        "\n",
        "        # Perform face detection using MTCNN\n",
        "        detections = detector.detect_faces(image)\n",
        "\n",
        "        # Initialize emotion counters\n",
        "        emotion_counts = {emotion: 0 for emotion in emotion_labels}\n",
        "\n",
        "        # Predict emotions for the detected faces\n",
        "        for detection in detections:\n",
        "            x, y, w, h = detection['box']\n",
        "            face_roi = image[y:y+h, x:x+w]\n",
        "            face = cv2.resize(face_roi, (48, 48))  # Resize to 48x48 for emotion recognition\n",
        "            face = cv2.cvtColor(face, cv2.COLOR_BGR2GRAY)  # Convert to grayscale\n",
        "            face = face.astype('float32') / 255.0  # Normalize pixel values\n",
        "            face = np.expand_dims(face, axis=0)  # Add batch dimension\n",
        "            face = np.expand_dims(face, axis=-1)  # Add channel dimension\n",
        "            prediction = model.predict(face)\n",
        "            emotion_index = np.argmax(prediction)\n",
        "            emotion = emotion_labels[emotion_index]\n",
        "            emotion_counts[emotion] += 1\n",
        "            cv2.putText(image, emotion, (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
        "            cv2.rectangle(image, (x, y), (x+w, y+h), (255, 0, 0), 2)\n",
        "\n",
        "        # Print emotion counts for the image\n",
        "        print(f\"Image: {filename}\")\n",
        "        for emotion, count in emotion_counts.items():\n",
        "            print(f\"Number of {emotion}: {count}\")\n",
        "\n",
        "        # Display the image with predicted emotions\n",
        "        cv2_imshow(image)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0Jm2rvF6_3xi"
      },
      "outputs": [],
      "source": [
        "# Enhanced Spatial Attention.\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization, Multiply, concatenate, Activation\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Define the Enhanced Spatial Attention layer\n",
        "class EnhancedSpatialAttention(tf.keras.layers.Layer):\n",
        "    def __init__(self, kernel_size=7):\n",
        "        super(EnhancedSpatialAttention, self).__init__()\n",
        "        self.kernel_size = kernel_size\n",
        "        self.conv1 = Conv2D(1, kernel_size=self.kernel_size, padding='same', activation='sigmoid')\n",
        "\n",
        "    def call(self, inputs):\n",
        "        avg_pool = tf.reduce_mean(inputs, axis=-1, keepdims=True)\n",
        "        max_pool = tf.reduce_max(inputs, axis=-1, keepdims=True)\n",
        "        concat = tf.concat([avg_pool, max_pool], axis=-1)\n",
        "        attention = self.conv1(concat)\n",
        "        return Multiply()([inputs, attention])\n",
        "\n",
        "# Load the FER2013 dataset\n",
        "df = pd.read_csv(\"/content/drive/MyDrive/emo/data/fer2013.csv\")\n",
        "\n",
        "# Extract images and labels\n",
        "pixels = df[\"pixels\"].tolist()\n",
        "images = []\n",
        "for pixel_sequence in pixels:\n",
        "    face = [int(pixel) for pixel in pixel_sequence.split(' ')]\n",
        "    face = np.array(face).reshape(48, 48, 1)  # Reshape to 48x48 grayscale image\n",
        "    images.append(face.astype('float32'))\n",
        "\n",
        "X = np.array(images)\n",
        "y = df[\"emotion\"].values\n",
        "\n",
        "# Normalize the pixel values\n",
        "X /= 255.0\n",
        "\n",
        "# Encode the labels\n",
        "label_encoder = LabelEncoder()\n",
        "y = label_encoder.fit_transform(y)\n",
        "y = tf.keras.utils.to_categorical(y, num_classes=7)\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define the enhanced FER model\n",
        "def Enhanced_FER_Model(input_shape=(48, 48, 1)):\n",
        "    visible = Input(shape=input_shape, name='input')\n",
        "    num_classes = 7\n",
        "\n",
        "    # The first block\n",
        "    conv1_1 = Conv2D(64, kernel_size=3, activation='relu', padding='same', name='conv1_1')(visible)\n",
        "    conv1_1 = BatchNormalization()(conv1_1)\n",
        "    attention1 = EnhancedSpatialAttention()(conv1_1)\n",
        "    conv1_2 = Conv2D(64, kernel_size=3, activation='relu', padding='same', name='conv1_2')(attention1)\n",
        "    conv1_2 = BatchNormalization()(conv1_2)\n",
        "    drop1_1 = Dropout(0.3, name='drop1_1')(conv1_2)\n",
        "\n",
        "    # The second block\n",
        "    conv2_1 = Conv2D(128, kernel_size=3, activation='relu', padding='same', name='conv2_1')(drop1_1)\n",
        "    conv2_1 = BatchNormalization()(conv2_1)\n",
        "    conv2_2 = Conv2D(128, kernel_size=3, activation='relu', padding='same', name='conv2_2')(conv2_1)\n",
        "    conv2_2 = BatchNormalization()(conv2_2)\n",
        "    attention2 = EnhancedSpatialAttention()(conv2_2)\n",
        "    drop2_1 = Dropout(0.3, name='drop2_1')(attention2)\n",
        "\n",
        "    # The third block\n",
        "    conv3_1 = Conv2D(256, kernel_size=3, activation='relu', padding='same', name='conv3_1')(drop2_1)\n",
        "    conv3_1 = BatchNormalization()(conv3_1)\n",
        "    conv3_2 = Conv2D(256, kernel_size=3, activation='relu', padding='same', name='conv3_2')(conv3_1)\n",
        "    conv3_2 = BatchNormalization()(conv3_2)\n",
        "    conv3_3 = Conv2D(256, kernel_size=3, activation='relu', padding='same', dilation_rate=2, name='conv3_3')(conv3_2)  # Dilated convolution\n",
        "    conv3_3 = BatchNormalization()(conv3_3)\n",
        "    attention3 = EnhancedSpatialAttention()(conv3_3)\n",
        "    drop3_1 = Dropout(0.3, name='drop3_1')(attention3)\n",
        "\n",
        "    # The fourth block\n",
        "    conv4_1 = Conv2D(256, kernel_size=3, activation='relu', padding='same', name='conv4_1')(drop3_1)\n",
        "    conv4_1 = BatchNormalization()(conv4_1)\n",
        "    conv4_2 = Conv2D(256, kernel_size=3, activation='relu', padding='same', name='conv4_2')(conv4_1)\n",
        "    conv4_2 = BatchNormalization()(conv4_2)\n",
        "    conv4_3 = Conv2D(256, kernel_size=3, activation='relu', padding='same', dilation_rate=2, name='conv4_3')(conv4_2)  # Dilated convolution\n",
        "    conv4_3 = BatchNormalization()(conv4_3)\n",
        "    attention4 = EnhancedSpatialAttention()(conv4_3)\n",
        "    drop4_1 = Dropout(0.3, name='drop4_1')(attention4)\n",
        "\n",
        "    # The fifth block\n",
        "    conv5_1 = Conv2D(512, kernel_size=3, activation='relu', padding='same', name='conv5_1')(drop4_1)\n",
        "    conv5_1 = BatchNormalization()(conv5_1)\n",
        "    conv5_2 = Conv2D(512, kernel_size=3, activation='relu', padding='same', name='conv5_2')(conv5_1)\n",
        "    conv5_2 = BatchNormalization()(conv5_2)\n",
        "    conv5_3 = Conv2D(512, kernel_size=3, activation='relu', padding='same', dilation_rate=2, name='conv5_3')(conv5_2)  # Dilated convolution\n",
        "    conv5_3 = BatchNormalization()(conv5_3)\n",
        "    attention5 = EnhancedSpatialAttention()(conv5_3)\n",
        "    drop5_1 = Dropout(0.3, name='drop5_1')(attention5)\n",
        "\n",
        "    # Flatten and output\n",
        "    flatten = Flatten(name='flatten')(drop5_1)\n",
        "    dense1 = Dense(256, activation='relu', name='dense1')(flatten)\n",
        "    drop6_1 = Dropout(0.5, name='drop6_1')(dense1)\n",
        "    dense2 = Dense(128, activation='relu', name='dense2')(drop6_1)\n",
        "    drop6_2 = Dropout(0.5, name='drop6_2')(dense2)\n",
        "    output = Dense(num_classes, activation='softmax', name='output')(drop6_2)\n",
        "\n",
        "    # Create model\n",
        "    model = Model(inputs=visible, outputs=output)\n",
        "\n",
        "    return model\n",
        "\n",
        "# Create an instance of the enhanced model\n",
        "model = Enhanced_FER_Model()\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(X_train, y_train, epochs=2, batch_size=32, validation_data=(X_test, y_test))\n",
        "# history = model.fit(X_train, y_train, epochs=1, batch_size=32, validation_data=(X_test, y_test))\n",
        "\n",
        "# Save the model\n",
        "model.save(\"/content/drive/MyDrive/emo/data/emotion_spatial.h5\")\n",
        "\n",
        "# Evaluate the model\n",
        "predictions = model.predict(X_test)\n",
        "y_pred = np.argmax(predictions, axis=1)\n",
        "\n",
        "# Print classification report and confusion matrix\n",
        "print(classification_report(np.argmax(y_test, axis=1), y_pred))\n",
        "print(confusion_matrix(np.argmax(y_test, axis=1), y_pred))\n",
        "\n",
        "# Plot accuracy and loss curves\n",
        "plt.plot(history.history['accuracy'], label='accuracy')\n",
        "plt.plot(history.history['val_accuracy'], label='val_accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "plt.plot(history.history['loss'], label='loss')\n",
        "plt.plot(history.history['val_loss'], label='val_loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zu2iiF92bfOs"
      },
      "outputs": [],
      "source": [
        "# Confusion matrix, IOU, TP, FP, FN - Enhanced Spatial Attention.\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import plotly.express as px\n",
        "import plotly.graph_objects as go\n",
        "\n",
        "# Load the FER2013 dataset\n",
        "df = pd.read_csv(\"/content/drive/MyDrive/emo/data/fer2013.csv\")\n",
        "\n",
        "# Extract images and labels\n",
        "pixels = df[\"pixels\"].tolist()\n",
        "images = []\n",
        "for pixel_sequence in pixels:\n",
        "    face = [int(pixel) for pixel in pixel_sequence.split(' ')]\n",
        "    face = np.array(face).reshape(48, 48, 1)  # Reshape to 48x48 grayscale image\n",
        "    images.append(face.astype('float32'))\n",
        "\n",
        "X = np.array(images)\n",
        "y = df[\"emotion\"].values\n",
        "\n",
        "# Normalize the pixel values\n",
        "X /= 255.0\n",
        "\n",
        "# Encode the labels\n",
        "label_encoder = LabelEncoder()\n",
        "y = label_encoder.fit_transform(y)\n",
        "y = tf.keras.utils.to_categorical(y, num_classes=7)\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Load the saved model\n",
        "model = tf.keras.models.load_model(\"/content/drive/MyDrive/emo/data/emotion_spatial.h5\")\n",
        "\n",
        "# Evaluate the model\n",
        "predictions = model.predict(X_test)\n",
        "y_pred = np.argmax(predictions, axis=1)\n",
        "\n",
        "# Print classification report and confusion matrix\n",
        "print(classification_report(np.argmax(y_test, axis=1), y_pred))\n",
        "conf_matrix = confusion_matrix(np.argmax(y_test, axis=1), y_pred)\n",
        "print(conf_matrix)\n",
        "\n",
        "# Calculate IoU for each class\n",
        "def calculate_iou(conf_matrix):\n",
        "    ious = []\n",
        "    num_classes = conf_matrix.shape[0]\n",
        "    tp_list, fp_list, fn_list = [], [], []\n",
        "    for c in range(num_classes):\n",
        "        TP = conf_matrix[c, c]\n",
        "        FP = np.sum(conf_matrix[:, c]) - TP\n",
        "        FN = np.sum(conf_matrix[c, :]) - TP\n",
        "        IoU = TP / (TP + FP + FN)\n",
        "        ious.append(IoU)\n",
        "        tp_list.append(TP)\n",
        "        fp_list.append(FP)\n",
        "        fn_list.append(FN)\n",
        "    return ious, tp_list, fp_list, fn_list\n",
        "\n",
        "ious, tp_list, fp_list, fn_list = calculate_iou(conf_matrix)\n",
        "for i, iou in enumerate(ious):\n",
        "    print(f\"IoU for class {i}: {iou}\")\n",
        "\n",
        "# Define emotion labels\n",
        "emotion_labels = label_encoder.classes_\n",
        "\n",
        "# Plot confusion matrix using Plotly\n",
        "fig_conf_matrix = px.imshow(conf_matrix,\n",
        "                            labels=dict(x=\"Predicted Label\", y=\"True Label\", color=\"Count\"),\n",
        "                            x=emotion_labels,\n",
        "                            y=emotion_labels,\n",
        "                            title=\"Confusion Matrix\")\n",
        "fig_conf_matrix.update_layout(coloraxis_showscale=True)\n",
        "fig_conf_matrix.show()\n",
        "\n",
        "# Plot TP, FP, FN using Plotly\n",
        "metrics_data = pd.DataFrame({\n",
        "    \"Class\": emotion_labels,\n",
        "    \"True Positives (TP)\": tp_list,\n",
        "    \"False Positives (FP)\": fp_list,\n",
        "    \"False Negatives (FN)\": fn_list\n",
        "})\n",
        "\n",
        "fig_metrics = go.Figure()\n",
        "fig_metrics.add_trace(go.Bar(x=metrics_data[\"Class\"], y=metrics_data[\"True Positives (TP)\"], name='TP'))\n",
        "fig_metrics.add_trace(go.Bar(x=metrics_data[\"Class\"], y=metrics_data[\"False Positives (FP)\"], name='FP'))\n",
        "fig_metrics.add_trace(go.Bar(x=metrics_data[\"Class\"], y=metrics_data[\"False Negatives (FN)\"], name='FN'))\n",
        "\n",
        "fig_metrics.update_layout(\n",
        "    barmode='group',\n",
        "    title=\"True Positives, False Positives, and False Negatives per Class\",\n",
        "    xaxis_title=\"Class\",\n",
        "    yaxis_title=\"Count\",\n",
        "    legend_title=\"Metrics\"\n",
        ")\n",
        "\n",
        "fig_metrics.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JusJOIrBbuB6"
      },
      "outputs": [],
      "source": [
        "# Detecting Group emotions - Enhanced Spatial Attention.\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import plotly.express as px\n",
        "import plotly.graph_objects as go\n",
        "\n",
        "# Load the FER2013 dataset\n",
        "df = pd.read_csv(\"/content/drive/MyDrive/emo/data/fer2013.csv\")\n",
        "\n",
        "# Extract images and labels\n",
        "pixels = df[\"pixels\"].tolist()\n",
        "images = []\n",
        "for pixel_sequence in pixels:\n",
        "    face = [int(pixel) for pixel in pixel_sequence.split(' ')]\n",
        "    face = np.array(face).reshape(48, 48, 1)  # Reshape to 48x48 grayscale image\n",
        "    images.append(face.astype('float32'))\n",
        "\n",
        "X = np.array(images)\n",
        "y = df[\"emotion\"].values\n",
        "\n",
        "# Normalize the pixel values\n",
        "X /= 255.0\n",
        "\n",
        "# Encode the labels\n",
        "label_encoder = LabelEncoder()\n",
        "y = label_encoder.fit_transform(y)\n",
        "y = tf.keras.utils.to_categorical(y, num_classes=7)\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Load the saved model\n",
        "model = tf.keras.models.load_model(\"/content/drive/MyDrive/emo/data/emotion_spatial.h5\")\n",
        "\n",
        "# Evaluate the model\n",
        "predictions = model.predict(X_test)\n",
        "y_pred = np.argmax(predictions, axis=1)\n",
        "\n",
        "# Print classification report and confusion matrix\n",
        "print(classification_report(np.argmax(y_test, axis=1), y_pred))\n",
        "conf_matrix = confusion_matrix(np.argmax(y_test, axis=1), y_pred)\n",
        "print(conf_matrix)\n",
        "\n",
        "# Calculate IoU for each class\n",
        "def calculate_iou(conf_matrix):\n",
        "    ious = []\n",
        "    num_classes = conf_matrix.shape[0]\n",
        "    tp_list, fp_list, fn_list = [], [], []\n",
        "    for c in range(num_classes):\n",
        "        TP = conf_matrix[c, c]\n",
        "        FP = np.sum(conf_matrix[:, c]) - TP\n",
        "        FN = np.sum(conf_matrix[c, :]) - TP\n",
        "        IoU = TP / (TP + FP + FN)\n",
        "        ious.append(IoU)\n",
        "        tp_list.append(TP)\n",
        "        fp_list.append(FP)\n",
        "        fn_list.append(FN)\n",
        "    return ious, tp_list, fp_list, fn_list\n",
        "\n",
        "ious, tp_list, fp_list, fn_list = calculate_iou(conf_matrix)\n",
        "for i, iou in enumerate(ious):\n",
        "    print(f\"IoU for class {i}: {iou}\")\n",
        "\n",
        "# Define emotion labels\n",
        "emotion_labels = label_encoder.classes_\n",
        "\n",
        "# Plot confusion matrix using Plotly\n",
        "fig_conf_matrix = px.imshow(conf_matrix,\n",
        "                            labels=dict(x=\"Predicted Label\", y=\"True Label\", color=\"Count\"),\n",
        "                            x=emotion_labels,\n",
        "                            y=emotion_labels,\n",
        "                            title=\"Confusion Matrix\")\n",
        "fig_conf_matrix.update_layout(coloraxis_showscale=True)\n",
        "fig_conf_matrix.show()\n",
        "\n",
        "# Plot TP, FP, FN using Plotly\n",
        "metrics_data = pd.DataFrame({\n",
        "    \"Class\": emotion_labels,\n",
        "    \"True Positives (TP)\": tp_list,\n",
        "    \"False Positives (FP)\": fp_list,\n",
        "    \"False Negatives (FN)\": fn_list\n",
        "})\n",
        "\n",
        "fig_metrics = go.Figure()\n",
        "fig_metrics.add_trace(go.Bar(x=metrics_data[\"Class\"], y=metrics_data[\"True Positives (TP)\"], name='TP'))\n",
        "fig_metrics.add_trace(go.Bar(x=metrics_data[\"Class\"], y=metrics_data[\"False Positives (FP)\"], name='FP'))\n",
        "fig_metrics.add_trace(go.Bar(x=metrics_data[\"Class\"], y=metrics_data[\"False Negatives (FN)\"], name='FN'))\n",
        "\n",
        "fig_metrics.update_layout(\n",
        "    barmode='group',\n",
        "    title=\"True Positives, False Positives, and False Negatives per Class\",\n",
        "    xaxis_title=\"Class\",\n",
        "    yaxis_title=\"Count\",\n",
        "    legend_title=\"Metrics\"\n",
        ")\n",
        "\n",
        "fig_metrics.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jBdAzE_HCCjR"
      },
      "outputs": [],
      "source": [
        "# Detecting Group emotions - Enhanced Spatial Attention.\n",
        "\n",
        "import cv2\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from mtcnn import MTCNN\n",
        "from google.colab.patches import cv2_imshow\n",
        "import os\n",
        "\n",
        "\n",
        "# Load the model with custom objects\n",
        "# model = tf.keras.models.load_model(\"/content/drive/MyDrive/emo/data/emotion_spatial.h5\", custom_objects={'SpatialAttention': SpatialAttention})\n",
        "model = tf.keras.models.load_model(\"/content/drive/MyDrive/emo/data/emotion_spatial.h5\")\n",
        "\n",
        "\n",
        "# Path to the folder containing images\n",
        "image_folder = \"/content/drive/MyDrive/emo/data\"\n",
        "\n",
        "# Initialize MTCNN for face detection\n",
        "detector = MTCNN()\n",
        "\n",
        "# Emotion labels\n",
        "emotion_labels = ['Angry', 'Disgust', 'Fear', 'Happy', 'Sad', 'Surprise', 'Neutral']\n",
        "\n",
        "# Iterate over files in the folder\n",
        "for filename in os.listdir(image_folder):\n",
        "    # Check if the file is an image file\n",
        "    if filename.endswith((\".jpg\", \".jpeg\", \".png\")):\n",
        "        # Construct the full path to the image file\n",
        "        image_path = os.path.join(image_folder, filename)\n",
        "\n",
        "        # Load the input image\n",
        "        image = cv2.imread(image_path)\n",
        "\n",
        "        # Perform face detection using MTCNN\n",
        "        detections = detector.detect_faces(image)\n",
        "\n",
        "        # Initialize emotion counters\n",
        "        emotion_counts = {emotion: 0 for emotion in emotion_labels}\n",
        "\n",
        "        # Predict emotions for the detected faces\n",
        "        for detection in detections:\n",
        "            x, y, w, h = detection['box']\n",
        "            face_roi = image[y:y+h, x:x+w]\n",
        "            face = cv2.resize(face_roi, (48, 48))  # Resize to 48x48 for emotion recognition\n",
        "            face = cv2.cvtColor(face, cv2.COLOR_BGR2GRAY)  # Convert to grayscale\n",
        "            face = face.astype('float32') / 255.0  # Normalize pixel values\n",
        "            face = np.expand_dims(face, axis=0)  # Add batch dimension\n",
        "            face = np.expand_dims(face, axis=-1)  # Add channel dimension\n",
        "            prediction = model.predict(face)\n",
        "            emotion_index = np.argmax(prediction)\n",
        "            emotion = emotion_labels[emotion_index]\n",
        "            emotion_counts[emotion] += 1\n",
        "            cv2.putText(image, emotion, (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
        "            cv2.rectangle(image, (x, y), (x+w, y+h), (255, 0, 0), 2)\n",
        "\n",
        "        # Print emotion counts for the image\n",
        "        print(f\"Image: {filename}\")\n",
        "        for emotion, count in emotion_counts.items():\n",
        "            print(f\"Number of {emotion}: {count}\")\n",
        "\n",
        "        # Display the image with predicted emotions\n",
        "        cv2_imshow(image)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eNDkVcCFhzLj"
      },
      "source": [
        "**Above model - 'Enhanced Spatial Attention (ESA)' crashed several time with the message - session crashed after using all available RAM / GPU.**"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
